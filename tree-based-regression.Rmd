---
title: "Analisis Prediktif Harga Rumah Menggunakan Tree Based Algorith"
author: "Moh. Rosidi"
date: "7/20/2020"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
    df_print: paged
    theme: yeti
    highlight: textmate
    css: assets/style.css
  pdf_document:
    toc: yes
    toc_depth: '3'
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Pengantar



# Dataset Ames



# Persiapan {.tabset}

## Library

```{r import-lib, cache=TRUE}
# library pembantu
library(plyr)
library(e1071)
library(foreach)
library(import)
library(tidyverse)
library(rsample)
library(recipes)
library(DataExplorer)
library(skimr)
library(modeldata)

# library model
library(caret) 
library(rpart)
library(ipred)
library(randomForest)
library(gbm)

# paket penjelasan model
library(rpart.plot)  
library(vip)         
```


## Import Dataset

```{r import-data, cache=TRUE}
data("ames")
```


# Data Splitting

```{r data-split, cache=TRUE}
set.seed(123)

split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


```{r target-vis, cache=TRUE}
# training set
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_density() 
# test set
ggplot(ames_test, aes(x = Sale_Price)) + 
  geom_density() 
```


# Analisis Data Eksploratif

## Ringkasan Data

```{r glimpse, cache=TRUE}
glimpse(ames_train)
```

```{r skim, cache=TRUE}
skim(ames_train)
```

```{r missing-vis, cache=TRUE}
plot_missing(ames_train)
```


## Variasi

```{r hist, cache=TRUE}
plot_histogram(ames_train, ncol = 2L, nrow = 2L)
```

```{r bar, cache=TRUE}
plot_bar(ames_train, ncol = 2L, nrow = 2L)
```

```{r nzv, cache=TRUE}
nzvar <- nearZeroVar(ames_train, saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv)
nzvar
```

```{r wt-nzv, cache=TRUE}
without_nzvar <- select(ames_train, !nzvar$rowname)
skim(without_nzvar)
```

```{r count-nominal, cache=TRUE}
# MS_SubClass 
count(ames_train, MS_SubClass) %>% arrange(n)
# Neighborhood
count(ames_train, Neighborhood) %>% arrange(n)
# Neighborhood
count(ames_train, Exterior_1st) %>% arrange(n)
# Exterior_2nd
count(ames_train, Exterior_2nd) %>% arrange(n)
```

```{r, cache=TRUE}
plot_bar(without_nzvar)
```


## Kovarian

```{r heatmap, cache=TRUE}
plot_correlation(ames_train, type = "continuous", 
                 cor_args = list(method = "spearman"))
```


# Target and Feature Engineering

```{r preprocess, cache=TRUE}
blueprint <- recipe(Sale_Price ~., data = ames_train) %>%
  # feature filtering
  step_nzv(all_nominal()) %>%
  # label encoding
  step_integer(dplyr::matches("Exterior|Neighbor|Sub|Qual|Cond|QC|Qu|Type")) %>%
  # standardization
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
blueprint
```

```{r prep, cache=TRUE}
prepare <- prep(blueprint, training = ames_train)
prepare
```

```{r baked, cache=TRUE}
baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)
baked_train
```

```{r, cache=TRUE}
skim(baked_train)
```


# Decision Tree Model

*Tree-based models* adalah kelas algoritma nonparametrik yang bekerja dengan mempartisi ruang fitur ke sejumlah daerah yang lebih kecil (tidak tumpang tindih) dengan nilai respons yang sama menggunakan seperangkat *aturan pemisahan*. Prediksi diperoleh dengan memasang model yang lebih sederhana (misal: Konstanta seperti nilai respons rata-rata) di setiap wilayah. Metode membagi dan menaklukkan seperti itu dapat menghasilkan aturan sederhana yang mudah ditafsirkan dan divisualisasikan dengan diagram pohon. 

Ada banyak metode yang dapat digunakan membangun pohon regresi, tetapi salah satu yang tertua dikenal sebagai pendekatan pohon klasifikasi dan regresi (CART) yang dikembangkan oleh [Breiman et al. (1984)](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418). Tutorial ini berfokus pada bagian regresi CART. Pohon regresi dasar mempartisi data yang ditetapkan ke dalam subkelompok yang lebih kecil dan kemudian melakukan fitting konstanta sederhana untuk setiap pengamatan dalam subkelompok. Partisi dicapai dengan partisi biner berturut-turut (alias partisi rekursif) berdasarkan pada berbagai prediktor. Konstanta untuk memprediksi didasarkan pada nilai respons rata-rata untuk semua pengamatan yang termasuk dalam subkelompok tersebut.

Sebagai contoh, misalkan kita ingin memprediksi mil per galon mobil rata-rata berdasarkan ukuran silinder (`cyl`) dan tenaga kuda (`hp`). Semua pengamatan melalui pohon ini, dinilai pada simpul tertentu, dan lanjutkan ke kiri jika jawabannya "ya" atau lanjutkan ke kanan jika jawabannya "tidak". Jadi, pertama, semua pengamatan yang memiliki 6 atau 8 silinder pergi ke cabang kiri, semua pengamatan lainnya dilanjutkan ke cabang kanan. Selanjutnya, cabang kiri selanjutnya dipartisi oleh tenaga kuda. Pengamatan 6 atau 8 silinder dengan tenaga kuda yang sama atau lebih besar dari 192 dilanjutkan ke cabang kiri; mereka yang kurang dari 192 hp melanjutkan ke kanan. Cabang-cabang ini mengarah ke *terminal node* atau *leaf nodes* yang berisi nilai respons prediksi kita. Pada dasarnya, semua pengamatan (mobil dalam contoh ini) yang tidak memiliki 6 atau 8 silinder (cabang paling kanan) rata-rata 27 mpg. Semua pengamatan yang memiliki 6 atau 8 silinder dan memiliki lebih dari 192 hp (cabang paling kiri) rata-rata 13 mpg.

![Prediksi mpg berdasarkan variabel cyl dan hp (Sumber: <http://uc-r.github.io/>)](http://uc-r.github.io/public/images/analytics/regression_trees/ex_regression_tree.png)

Contoh sederhana tersebut dapat kita generalisasikan. Variabel respon kontinu $Y$ dan dua buah variabel input $X_1$ dan $X_2$. Partisi rekursif menghasilkan tiga buah area (*nodes*), yaitu: $R_1$, $R_2$, dan $R_3$ dimana model memprediksi $Y$ dengan sebuah konstanta $c_m$ pada area $R_m$:

$$
\hat{f}\left(X\right) = \sum_{m=1}^{3} c_{m} I\left(X_1,X_2\right) \in R_m
$$

## Menentukan Split pada Decision Tree

Pertama, penting untuk mewujudkan partisi variabel yang dilakukan secara *top-down*. Ini hanya berarti bahwa partisi yang dilakukan sebelumnya pada pohon yang terbentuk tidak akan berubah oleh partisi selanjutnya. Tetapi bagaimana partisi ini dibuat? Model dimulai dengan seluruh data,$S$, dan mencari setiap nilai berbeda dari setiap variabel input untuk menemukan prediktor dan nilai *split* yang membagi data menjadi dua area ($R_1$ dan $R_2$) sedemikian rupa sehingga jumlah kesalahan kuadrat keseluruhan diminimalkan:

$$
minimize \left{SSE = \sum_{i \in R_1}^{ } \left(y_i-c_1\right)^2 +  \sum_{i \in R_2}^{ } \left(y_i-c_2\right)^2\right}
$$

Setelah menemukan  *split* terbaik, kita mempartisi data menjadi dua area yang dihasilkan dan mengulangi proses *split* pada masing-masing dua area Proses ini berlanjut sampai kriteria penghentian tercapai. Pohon yang dihasilkan biasanya sangat dalam, kompleks yang dapat menghasilkan prediksi yang baik pada data *training*, tetapi kemungkinan besar model yang dibuat *overfiting* dan akan menghasilkan hasil prediksi yang buruk pada data *test*.

## Validasi Silang dan Parameter Tuning

```{r, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  search = "random",
  # repeats = 5,
  allowParallel = TRUE
)
```


```{r, cache=TRUE}
system.time(
dt_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "rpart",
  trControl = cv,
  tuneLength = 10,
  metric = "RMSE"
  )
)

dt_fit_cv
```

```{r dt-rmse, cache=TRUE}
dt_rmse <- dt_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

dt_rmse
```


```{r dt-cv-vis, chace = TRUE}
# visualisasi
ggplot(dt_fit_cv)
```


## Model Akhir

```{r dt-final, cache=TRUE}
dt_fit <- dt_fit_cv$finalModel
```


```{r dt-vis, cache=TRUE}
# visualisasi
rpart.plot(dt_fit)
```

```{r dt-res-vis, cache=TRUE}
pred_train <- predict(dt_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sp <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point()
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sp, hs, ncol = 2)
```


```{r dt-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(dt_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```


## Tingkat Kepentingan Variabel

```{r dt-vip, cache=TRUE}
vip(dt_fit_cv, num_features = 10)
```


# Bagging

## Validasi Silang

```{r bag-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  # repeats = 5,
  allowParallel = TRUE
)
```


```{r bag-fit, cache=TRUE}
system.time(
bag_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "treebag",
  trControl = cv,
  importance = TRUE
  )
)

bag_fit_cv
```

```{r bag-rmse, cache=TRUE}
bag_rmse <- bag_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

bag_rmse
```

## Model AKhir


```{r bag-final, cache=TRUE}
bag_fit <- bag_fit_cv$finalModel
```


```{r bag-res-vis, cache=TRUE}
pred_train <- predict(bag_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point()
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sc, hs, ncol = 2)
```

```{r bag-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(bag_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Tingkat Kepentingan Variabel

```{r bag-vip, cache=TRUE}
vip(bag_fit_cv, num_features = 10)
```

# Random Forest

## Validasi Silang dan Parameter Tuning

```{r rf-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  search = "random",
  # repeats = 5,
  allowParallel = TRUE
)
```

```{r rf-grid, cache=TRUE}
hyper_grid <- expand.grid(
  mtry = seq(10, 30, by = 2)
)
```

```{r rf-fit, cache=TRUE}
# membuat model
system.time(
rf_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "parRF",
  trControl = cv,
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
)

rf_fit_cv
```

```{r rf-rmse, cache=TRUE}
rf_rmse <- rf_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

rf_rmse
```

```{r rf-vis, chace = TRUE}
# visualisasi
ggplot(rf_fit_cv)
```

## Model AKhir

```{r rf-final, cache=TRUE}
rf_fit <- rf_fit_cv$finalModel
```

```{r rf-resid-vis, cache=TRUE}
pred_train <- predict(rf_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point() 
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sp, hs, ncol = 2)
```

```{r rf-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(rf_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Tingkat Kepentingan Variabel

```{r rf-vip, cache=TRUE}
vip(rf_fit, num_features = 10)
```

# Boosting

## Validasi Silang dan Parameter Tuning

```{r boost-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10, 
  # repeats = 5,
  allowParallel = TRUE
)
```

```{r boost-grid, cache=TRUE}
hyper_grid <- expand.grid(
  n.trees = 6000,
  shrinkage = 0.01,
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15)
)
```

```{r boost-fit, cache=TRUE}
system.time(
gb_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "gbm",
  trControl = cv,
  tuneGrid = hyper_grid,
  verbose = FALSE,
  metric = "RMSE"
  )
)

gb_fit_cv
```

```{r boost-rmse, cache=TRUE}
boost_rmse <- gb_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

boost_rmse
```

```{r boost-vis, cache=TRUE}
ggplot(gb_fit_cv)
```


## Model Akhir

```{r boost-final, cache=TRUE}
gb_fit <- gb_fit_cv$finalModel
```


```{r boost-resid, cache=TRUE}
pred_train <- predict(gb_fit, n.trees = gb_fit$n.trees,
                      baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point() 
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sp, hs, ncol = 2)
```

```{r boost-test-rmse, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(gb_fit, n.trees = gb_fit$n.trees,
                     baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Tingkat Kepentingan Variabel

```{r boost-vip, cache=TRUE}
vip(gb_fit_cv, num_features = 10)
```




# Ringkasan Model


```{r table, cache=TRUE}
table <- tibble(Model = c("Decision Tree", "Bagging", "Random Forest", "Gradient Boosting"),
                RMSE = c(dt_rmse, bag_rmse, rf_rmse, boost_rmse))

table
```


