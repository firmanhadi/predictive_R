---
title: "Analisis Prediktif Harga Rumah Menggunakan Tree Based Algorithm"
author: "Moh. Rosidi"
date: "7/22/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Pengantar



# Dataset Ames



# Persiapan {.tabset}

## Library

```{r import-lib}
# library pembantu
library(plyr)
library(e1071)
library(foreach)
library(import)
library(tidyverse)
library(rsample)
library(recipes)
library(DataExplorer)
library(skimr)
library(modeldata)

# library model
library(caret) 
library(rpart)
library(ipred)
library(randomForest)
library(gbm)

# paket penjelasan model
library(rpart.plot)  
library(vip)
library(pdp)
```


## Import Dataset

```{r import-data}
data("ames")
```


# Data Splitting

```{r data-split}
set.seed(123)

split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


```{r target-vis}
# training set
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_density() 
# test set
ggplot(ames_test, aes(x = Sale_Price)) + 
  geom_density() 
```


# Analisis Data Eksploratif

## Ringkasan Data

```{r glimpse}
glimpse(ames_train)
```

```{r skim}
skim(ames_train)
```

```{r missing-vis}
plot_missing(ames_train)
```


## Variasi

```{r hist}
plot_histogram(ames_train, ncol = 2L, nrow = 2L)
```

```{r bar}
plot_bar(ames_train, ncol = 2L, nrow = 2L)
```

```{r nzv}
nzvar <- nearZeroVar(ames_train, saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv)
nzvar
```

```{r wt-nzv}
without_nzvar <- select(ames_train, !nzvar$rowname)
skim(without_nzvar)
```

```{r count-nominal}
# MS_SubClass 
count(ames_train, MS_SubClass) %>% arrange(n)
# Neighborhood
count(ames_train, Neighborhood) %>% arrange(n)
# Neighborhood
count(ames_train, Exterior_1st) %>% arrange(n)
# Exterior_2nd
count(ames_train, Exterior_2nd) %>% arrange(n)
```

```{r}
plot_bar(without_nzvar)
```


## Kovarian

```{r heatmap}
plot_correlation(ames_train, type = "continuous", 
                 cor_args = list(method = "spearman"))
```


# Target and Feature Engineering

```{r preprocess}
blueprint <- recipe(Sale_Price ~., data = ames_train) %>%
  # feature filtering
  step_nzv(all_nominal()) %>%
  # label encoding
  step_integer(dplyr::matches("Exterior|Neighbor|Sub|Qual|Cond|QC|Qu|Type")) %>%
  # standardization
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
blueprint
```

```{r prep}
prepare <- prep(blueprint, training = ames_train)
prepare
```

```{r baked}
baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)
baked_train
```

```{r}
skim(baked_train)
```


# Random Forest

Bagging (agregasi bootstrap) adalah teknik yang dapat mengubah model pohon tunggal dengan varian tinggi dan kemampuan prediksi yang buruk menjadi fungsi prediksi yang cukup akurat. Sayangnya, bagging biasanya kekurangan, yiatu: adanya korelasi pada tiap pohon yang mengurangi kinerja keseluruhan model. *Random forest* adalah modifikasi bagging yang membangun koleksi besar pohon yang tidak berkorelasi dan telah menjadi algoritma pembelajaran “out-of-the-box” yang sangat populer yang dengan kinerja prediksi yang baik. 

*Random forest* dibangun di atas prinsip-prinsip dasar yang sama seperti *decision tress* dan bagging. Bagging memperkenalkan komponen acak ke dalam proses pembangunan pohon yang mengurangi varian prediksi pohon tunggal dan meningkatkan kinerja prediksi. Namun, pohon-pohon di bagging tidak sepenuhnya independen satu sama lain karena semua prediktor asli dianggap di setiap split setiap pohon. Sebaliknya, pohon dari sampel bootstrap yang berbeda biasanya memiliki struktur yang mirip satu sama lain (terutama di bagian atas pohon) karena hubungan yang mendasarinya.

Sebagai contoh, jika kita membuat enam pohon keputusan dengan sampel bootstrap data perumahan Boston yang berbeda, kita melihat bahwa puncak pohon semua memiliki struktur yang sangat mirip. Meskipun ada 15 variabel prediktor untuk dipecah, keenam pohon memiliki kedua variabel lstat dan rm yang mendorong beberapa split pertama.

Sebagai contoh, jika kita membuat enam *decision trees* dengan sampel bootstrap [data perumahan Boston](http://uc-r.github.io/(http://lib.stat.cmu.edu/datasets/boston)) yang berbeda, kita melihat bahwa puncak pohon semua memiliki struktur yang sangat mirip. Meskipun ada 15 variabel prediktor untuk dipecah, keenam pohon memiliki kedua variabel `lstat` dan `rm` yang mendorong beberapa split pertama.

![Enam decision trees berdasarkan sampel bootsrap yang berbeda-beda](http://uc-r.github.io/public/images/analytics/random_forests/tree-correlation-1.png)

Karakteristik ini dikenal sebagai **korelasi pohon** dan mencegah bagging dari secara optimal mengurangi varians dari nilai-nilai prediktif. Untuk mengurangi varian lebih lanjut, kita perlu meminimalkan jumlah korelasi antar pohon-pohon tersebut. Ini bisa dicapai dengan menyuntikkan lebih banyak keacakan ke dalam proses penanaman pohon. *Random Forest* mencapai ini dalam dua cara:

1. **Bootstrap**: mirip dengan bagging, setiap pohon ditumbuhkan ke set data *bootstrap resampled*, yang membuatnya berbeda dan agak mendekorelasi antar pohon tersebut.
2. **Split-variable randomization**: setiap kali pemisahan dilakukan, pencarian untuk variabel terbagi terbatas pada subset acak $m$ dari variabel $p$. Untuk pohon regresi, nilai default tipikal adalah $m = p/3$ tetapi ini harus dianggap sebagai *parameter tuning*. Ketika $m = p$, jumlah pengacakan hanya menggunakan langkah 1 dan sama dengan bagging.

Algoritma dasar dari *random forest* adalah sebagai berikut:

```
1.  Diberikan set data training
2.  Pilih jumlah pohon yang akan dibangun (n_trees)
3.  for i = 1 to n_trees do
4.  | Hasilkan sampel bootstrap dari data asli
5.  | Tumbuhkan pohon regresi / klasifikasi ke data yang di-bootstrap
6.  | for each split do
7.  | | Pilih variabel m_try secara acak dari semua variabel p
8.  | | Pilih variabel / titik-split terbaik di antara m_try
9.  | | Membagi node menjadi dua node anak
10. | end
11. | Gunakan kriteria berhenti model pohon biasa untuk menentukan 
    | kapan pohon selesai (tapi jangan pangkas)
12. end
13. Output ensemble of trees 
```

Karena algoritma secara acak memilih sampel bootstrap untuk dilatih dan prediktor digunakan pada setiap split, korelasi pohon akan berkurang melebihi bagging.

## OOB Error vs Test Set Error

Mirip dengan bagging, manfaat alami dari proses *bootstrap resampling* adalah *randomforest* memiliki sampel *out-of-bag* (OOB) yang memberikan perkiraan kesalahan pengujian yang efisien dan masuk akal. Ini memberikan satu set validasi bawaan tanpa kerja ekstra , dan kita tidak perlu mengorbankan data *training* apa pun untuk digunakan untuk validasi. Ini membuat proses identifikasi jumlah pohon yang diperlukan untuk menstabilkan tingkat kesalahan selama proses *tuning* menjadi lebih efisien; Namun, seperti yang diilustrasikan di bawah ini, beberapa perbedaan antara kesalahan OOB dan kesalahan tes diharapkan.

![Random forest OOB vs validation error (Sumber: http://uc-r.github.io/)](http://uc-r.github.io/public/images/analytics/random_forests/oob-error-compare-1.svg)

Selain itu, banyak paket tidak melacak pengamatan mana yang merupakan bagian dari sampel OOB untuk pohon tertentu dan yang tidak. Jika kita membandingkan beberapa model dengan yang lain, kita ingin membuat skor masing-masing pada set validasi yang sama untuk membandingkan kinerja. Selain itu, meskipun secara teknis dimungkinkan untuk menghitung metrik tertentu seperti *root mean squared logarithmic error* (RMSLE) pada sampel OOB, itu tidak dibangun untuk semua paket. Jadi jika kita ingin membandingkan beberapa model atau menggunakan fungsi *loss* yang sedikit lebih tradisional, kita mungkin ingin tetap melakukan validasi silang.

## Kelebihan dan Kekurangan

**Kelbihan**

* Biasanya memiliki kinerja yang sangat bagus
* “*Out-of-the-box*” yang luar biasa bagus - sangat sedikit penyesuaian yang diperlukan
* Kumpulan validasi bawaan - tidak perlu mengorbankan data untuk validasi tambahan
* Tidak diperlukan pra-pemrosesan
* Bersifat *robust* dengan adanya *outlier*

**Kekurangan**

* Dapat menjadi lambat pada set data besar
* Meskipun akurat, seringkali tidak dapat bersaing dengan algoritma *boosting*
* Kurang mudah untuk ditafsirkan


## Validasi Silang dan Parameter Tuning

Pada fungsi `trainControl()` argumen yang digunakan sama dengan model bagging. 

```{r rf-cv}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  # repeats = 5,
  allowParallel = TRUE
)
```

Pada proses *training*, kita akan melakukan *parameter tuning* menggunakan metode *grid search*. Parameter yang akan dilakukan tuning pada model ini adalah `mtry` yang merupakan parameter *split-variable randomization*.

```{r rf-grid}
hyper_grid <- expand.grid(
  mtry = seq(10, 30, by = 2)
)
```

Pada proses training, `method` yang digunakan adalah `parRF` atau *parallel random forest*. Metode ini memerlukan sejumlah paket tambahan untuk memastikan proses parallel dapat berjalan, seperti: `e1071`, `randomForest`, `plyr`, dan `import`.

```{r rf-fit}
# membuat model
system.time(
rf_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "parRF",
  trControl = cv,
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
)

rf_fit_cv
```

Proses *training* berlangsung selama 1780.221 detik dengan 11 model terbentuk. Dari seluruh model tersebut, model dengan parameter `mtry = 28` memiliki rata-rata **RMSE** yang paling baik. Untuk dapat mengakses **RMSE** model terbaik, jalankan sintaks berikut:

```{r rf-rmse}
rf_rmse <- rf_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

rf_rmse
```

Nilai **RMSE** model *random forest* yang dihasilkan jauh lebih baik dibandingkan dua model awal. Reduksi terhadap jumlah pohon yang saling berkorelasi telah meningkatkan performa model secara signifikan.

Berikut adalah ringkasan performa masing-masing model:

```{r rf-vis, chace = TRUE}
# visualisasi
ggplot(rf_fit_cv)
```

## Model AKhir

Untuk mengekstrak model final, jalankan sintaks berikut:

```{r rf-final}
rf_fit <- rf_fit_cv$finalModel
```

Untuk mengeceke performa model dalam melakukan prediksi, kita dapat mengecek plot residual model tersebut.

```{r rf-resid-vis}
pred_train <- predict(rf_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point() 
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sc, hs, ncol = 2)
```

Performa prediksi model mengalami oeningkatan dibanding dua model sebelumnya yang ditunjukkan adanya reduksi dari pola heterkodestisitas pada plot yang dihasilkan.

Untuk mengecek performa prediksi model pada dataset baru (data *test*), jalankan sintaks berikut:

```{r rf-rmse-test}
# prediksi Sale_Price ames_test
pred_test <- predict(rf_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Interpretasi Fitur

Untuk mengetahui variabel apa yang paling berpengaruh terhadap performa model, kita dapat menggunakan visualisasi *variabel importance plot*.

```{r rf-vip}
vip(rf_fit, num_features = 10)
```

Berdasarkan visualisasi tersebut, terdapat tiga buah variabel yang memiliki nilai kepentingan yang tinggi, yaitu: `Garage_Cars`, `Year_Built`, dan `Gr_Liv_Area`. Untuk mengetahui efek dari ketiga variabel tersebut terhadap kemampuan prediksi model, jalankan sintaks berikut:

```{r rf-pdp}
p1 <- pdp::partial(rf_fit_cv, pred.var = "Garage_Cars") %>% autoplot()
p2 <- pdp::partial(rf_fit_cv, pred.var = "Year_Built") %>% autoplot()
p3 <- pdp::partial(rf_fit_cv, pred.var = "Gr_Liv_Area") %>% autoplot()


gridExtra::grid.arrange(p1, p2, p3, 
                        ncol=2)
```


Berdasarkan output yang dihasilkan, ketiga variabel memiliki relasi non-linier terhadap variabel target.